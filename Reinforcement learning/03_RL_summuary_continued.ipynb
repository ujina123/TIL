{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+Q7jCFeIDL6aTBz2UGQ0M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["23.05.17\n","\n","## 강화학습\n","- 사용하는 경우 : 지도학습으로 할 수 없는 경우 ex) 데이터가 없을 경우, ...\n","\n","### 필요한 개념\n","- Agent : 행위의 주체 ex) 사람, 헬기, 로봇, ...\n","    - 주체는 어떠한 액션을 취한다. ex) 사람이 걷는다, 뛴다, ...\n","- Env : 환경 \n","    - 주체가 액션을 취하는 환경 \n","- Feedback : Agent가 Env에서 Action을 취할때 주는 피드백!\n","    - state : 상태\n","        - 상태 값은 길어질 수 있다. ex) 회전, 좌표, 기울기 등등\n","    - reword : 보상 \n","\n","### 목표 \n","- 현재 상태일때, 가장 적절한 행동을 취할수 있도록 학습하는 것! \n","- state-action function : Q(s,a) = R \n","    - 지금 상태(s)에 어떤 행동(a)을 취하면 궁극적으로 좋을까(R)?\n","\n","### Bellman Equation \n","- 현재는 미래에 대한 함수이다. \n","- state-action function : Q(s,a) = R \n","- Q(S,A) = R(S) + r * Q(S',A')\n","    - S: 상태 \n","    - A: 행동\n","    - R(S): 즉각적인 보상\n","    - r: 가중치 (gamma) : 0~1,loss factor\n","        - 1에 가까워질수록 최초 액션이 중요하다는 것을 알수있다.\n","    - S': 다음 상태\n","    - A': 다음 행동\n","- (S, A, R(S), S') 데이터를 모두 만들어서 데이터셋을 구축한다. 그래서 데이터 샘플링이 중요하다.\n","\n","### DQN (Deep Q Network)\n","- 강화학습의 식들을 풀어내서 구현하기가 어려워서 모델링을 하는 방식\n","- Q function을 근사시키는 모델링\n","\n","### Reword Function \n","- 최종 보상 함수를 사용자가 직접 만들어서 사용\n","\n","### Exploration\n","- 탐험하다. \n","- Q(s,a) 값이 95%가 되도록 만드는 것.\n","- 원래 그대로 하는 것.\n","\n","### Exploitation\n","- 이용하다. 착취하다. 집중하다\n","- Q(s,a) 값이 5%가 되도록 만드는 것\n","\n","### ε-greedy Policy\n","- 학습하면서 얻은 가치 함수 중 가장 큰 확률 값(ε=0.95)을 따라가는 것\n","\n","### mini batch\n","- GD : Gradient Descent 경사하강법\n","- SGD : Stochasic Gradient Descent (mini batch를 사용) \n","    - update 할 분량의 평균으로 가중치 업데이트\n","- 데이터를 한번에 계산하는 것이 아니라 조금씩 나눠서 진행하는 것\n","\n","### soft update\n","- 새로운 것에는 가중치를 적게 주고, 기존의 값에 가중치를 더 주는 방식\n","- lr과는 다르다.\n","- W = 0.01Wnew + 0.99W\n","- B = 0.01Bnew + 0.99W\n","\n","- 상수가 곱해지는 것은 속도에 큰 차이가 없지만, 변수가 곱해지는 연수는 속도가 더 늘어난다."],"metadata":{"id":"3rXp0ZBth2dU"}}]}