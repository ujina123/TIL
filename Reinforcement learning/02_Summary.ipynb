{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmZ1UOSeJyB7Ff6VQbbTf7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["2023.04.26\n","\n","## 강화학습 (reinforcement)\n","- 강화학습의 목표 : policy를 찾는 것\n","    - policy : 어떤 상태일때, 어떤 행동을 해야하는지 `π(s) = a` \n","    - `Q(s,a) = return` : 지금 상태(s)일때, 어떤 행동(a)을 했을때의 값 ( = state-action value function)<br>\n","        ex) s = 바람이 왼쪽에서 불어옴 , a = 왼쪽/오른쪽/위/아래로 움직임으로 가장 최적의 return값을 찾는다.\n","    - `보상(reward)` : 행동(a)에 대해 순간적으로 주어지는 값\n","    - `return` : 궁극적인 최종 값, 첫 step을 어떻게 하는지에 따라 결과가 달라질 수 있다.\n","    - Q function이 어떻게 학습되는지가 중요한 task.\n","    - `E[Q()]` = Q값의 기대값 (=average 평균)\n","\n","## Markov Decision Process (MDP)\n","- 지금 행동(a)이 다음 행동에 어떤 영향을 미치는지\n","\n","## Bellman Equation\n","- `return`을 계산하는 방식을 정의할때 사용되는 방식.\n","- Q(s,a) = reward you get right away + r(gamma) * return from behaving optimally starting from state s'\n","- 재귀함수의 개념과 비슷 ? \n","- 순서가 있는 데이터를 가지고 최종적인 값으로 평가하는 것.\n","- ex) 화폐 가치 계산\n","\n","## Discrete vs Continuous State\n","- 상태(s) : 포지션(x,y), 각도, 포지션의 증가분(속도), 높이, 기울기 등의 여러가지가 포함될 수 있다.\n","- `Translation`와 `Rotation` (움직임을 수학적으로 표현)\n","    - `Translation` : x축, y축, z축\n","    - `Rotation` : roll, pitch, yaw\n","\n","## Deep Reinforcement Learning (DRL)\n","- 머신러닝 기법 중 하나로, 심층 강화학습이라고 한다.\n","-\n","\n","\n"],"metadata":{"id":"HWjT7hRbgfIB"}},{"cell_type":"code","source":[],"metadata":{"id":"ZZuxSMAZsMsM"},"execution_count":null,"outputs":[]}]}